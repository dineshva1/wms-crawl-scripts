#!/usr/bin/env python3
"""
S3 Output Verification Script
Comprehensive verification of all output files generated by the WMS crawl scripts
"""

import boto3
import os
from datetime import datetime
from dotenv import load_dotenv
from typing import Dict, List, Tuple

def verify_s3_outputs(date_suffix: str = None) -> Dict[str, any]:
    """
    Verify all S3 outputs for a given date
    
    Args:
        date_suffix: Date in YYYYMMDD format, defaults to today
        
    Returns:
        Dictionary with verification results
    """
    load_dotenv()
    
    if not date_suffix:
        date_suffix = datetime.now().strftime("%Y%m%d")
    
    # Initialize S3 client
    s3_client = boto3.client(
        's3',
        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
        region_name=os.getenv('AWS_REGION')
    )
    
    bucket_name = os.getenv('BUCKET_NAME')
    
    print(f"🔍 COMPREHENSIVE S3 VERIFICATION FOR {date_suffix}")
    print("=" * 70)
    
    verification_results = {
        'date': date_suffix,
        'bucket': bucket_name,
        'raw_files': {},
        'processed_files': {},
        'excel_files': {},
        'total_files': 0,
        'total_size_mb': 0,
        'success': True,
        'errors': []
    }
    
    # Define expected file patterns and locations
    expected_files = {
        'raw_files': {
            'rzn1/order_summary/raw': [
                f'ORDER_SUMMARY{date_suffix}.csv',
                f'SALES_RETURN{date_suffix}.csv'
            ],
            'rzn1/inventory_summary/raw': [
                f'BATCH_LEVEL_INVENTORY{date_suffix}.csv',
                f'OPEN_ORDER_SUMMARY{date_suffix}.csv', 
                f'CLOSING_STOCK{date_suffix}.csv'
            ]
        },
        'processed_files': {
            'rzn1/order_summary/processed': [
                f'ORDER_SUMMARY_COMPLETE{date_suffix}.csv',
                f'ORDER_SUMMARY_UP{date_suffix}.csv',
                f'ORDER_SUMMARY_HR{date_suffix}.csv'
            ],
            'rzn1/inventory_summary/processed': [
                f'INVENTORY_SUMMARY_COMPLETE{date_suffix}.csv',
                f'CLOSINGSTOCK_UP{date_suffix}.csv',
                f'CLOSINGSTOCK_HR{date_suffix}.csv'
            ]
        },
        'excel_files': {
            'rzn1/order_summary/report/main/sales': [
                # Excel files follow monthly pattern
                f'{datetime.now().strftime("%b")}_Sales_Data_{datetime.now().year}.xlsx'
            ]
        }
    }
    
    def check_files_in_prefix(prefix: str, expected_files: List[str], category: str) -> Tuple[List[str], List[str]]:
        """Check if expected files exist in S3 prefix"""
        try:
            full_prefix = f"{prefix}/{date_suffix}/" if category != 'excel_files' else prefix + "/"
            response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=full_prefix)
            
            found_files = []
            missing_files = []
            
            if 'Contents' in response:
                existing_files = [obj['Key'].split('/')[-1] for obj in response['Contents']]
                file_details = {obj['Key'].split('/')[-1]: obj for obj in response['Contents']}
                
                for expected_file in expected_files:
                    if expected_file in existing_files:
                        found_files.append(expected_file)
                        file_info = file_details[expected_file]
                        size_mb = file_info['Size'] / (1024 * 1024)
                        verification_results['total_size_mb'] += size_mb
                        verification_results['total_files'] += 1
                        
                        print(f"  ✅ {expected_file} - {size_mb:.2f} MB - {file_info['LastModified']}")
                    else:
                        missing_files.append(expected_file)
                        print(f"  ❌ {expected_file} - MISSING")
            else:
                missing_files = expected_files
                for missing_file in missing_files:
                    print(f"  ❌ {missing_file} - MISSING (no files in prefix)")
                    
            return found_files, missing_files
            
        except Exception as e:
            error_msg = f"Error checking {prefix}: {str(e)}"
            verification_results['errors'].append(error_msg)
            print(f"  ❌ Error: {error_msg}")
            return [], expected_files
    
    # Check raw files
    print("\n📁 RAW INPUT FILES:")
    for prefix, files in expected_files['raw_files'].items():
        print(f"\n  📂 {prefix.upper()}:")
        found, missing = check_files_in_prefix(prefix, files, 'raw_files')
        verification_results['raw_files'][prefix] = {
            'found': found,
            'missing': missing,
            'success': len(missing) == 0
        }
        if missing:
            verification_results['success'] = False
    
    # Check processed files
    print("\n📊 PROCESSED OUTPUT FILES:")
    for prefix, files in expected_files['processed_files'].items():
        print(f"\n  📂 {prefix.upper()}:")
        found, missing = check_files_in_prefix(prefix, files, 'processed_files')
        verification_results['processed_files'][prefix] = {
            'found': found,
            'missing': missing,
            'success': len(missing) == 0
        }
        if missing:
            verification_results['success'] = False
    
    # Check Excel files
    print("\n📈 EXCEL REPORT FILES:")
    for prefix, files in expected_files['excel_files'].items():
        print(f"\n  📂 {prefix.upper()}:")
        found, missing = check_files_in_prefix(prefix, files, 'excel_files')
        verification_results['excel_files'][prefix] = {
            'found': found,
            'missing': missing,
            'success': len(missing) == 0
        }
        if missing:
            verification_results['success'] = False
    
    # Summary
    print(f"\n{'🎉' if verification_results['success'] else '⚠️'} VERIFICATION SUMMARY:")
    print("=" * 50)
    print(f"📅 Date: {date_suffix}")
    print(f"🗂️ Total Files: {verification_results['total_files']}")
    print(f"💾 Total Size: {verification_results['total_size_mb']:.2f} MB")
    print(f"✅ Status: {'SUCCESS' if verification_results['success'] else 'ISSUES FOUND'}")
    
    if verification_results['errors']:
        print(f"\n❌ Errors encountered:")
        for error in verification_results['errors']:
            print(f"  - {error}")
    
    return verification_results

def verify_workflow_completion(date_suffix: str = None) -> bool:
    """
    Quick verification that all three processors completed successfully
    
    Returns:
        True if all expected outputs are present
    """
    results = verify_s3_outputs(date_suffix)
    
    # Define minimum required files for successful workflow
    required_outputs = [
        'ORDER_SUMMARY_COMPLETE',
        'ORDER_SUMMARY_UP', 
        'ORDER_SUMMARY_HR',
        'INVENTORY_SUMMARY_COMPLETE',
        'CLOSINGSTOCK_UP',
        'CLOSINGSTOCK_HR'
    ]
    
    all_found_files = []
    for category in ['processed_files']:
        for prefix_data in results[category].values():
            all_found_files.extend(prefix_data['found'])
    
    missing_required = []
    for required in required_outputs:
        if not any(required in found_file for found_file in all_found_files):
            missing_required.append(required)
    
    if missing_required:
        print(f"\n⚠️ WORKFLOW INCOMPLETE - Missing required outputs:")
        for missing in missing_required:
            print(f"  - {missing}")
        return False
    
    print(f"\n🎯 WORKFLOW COMPLETE - All processors executed successfully!")
    return True

if __name__ == "__main__":
    import sys
    
    date_to_check = sys.argv[1] if len(sys.argv) > 1 else None
    
    # Run verification
    results = verify_s3_outputs(date_to_check)
    
    # Check workflow completion
    workflow_success = verify_workflow_completion(date_to_check)
    
    # Exit with appropriate code
    sys.exit(0 if results['success'] and workflow_success else 1)
